{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Street Names Dataset Extraction\n",
        "\n",
        "This notebook extracts unique street names from each city's original data and creates clean datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('data')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chicago\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'Chicago/original_data/Chicago_Street_Names_20251209.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['Full Street Name', 'Direction', 'Street ', 'Suffix'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'Street '\n",
        "chicago_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "chicago_streets = sorted([s for s in chicago_streets if s])\n",
        "print(f\"\\n✓ Chicago: {len(chicago_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dallas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'Dallas/original_data/SAN__STREET_LAYER.csv', low_memory=False)\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['PREFIX', 'NAME', 'TYPE', 'SUFFIX', 'FULLSTREET'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'FULLSTREET'\n",
        "dallas_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "dallas_streets = sorted([s for s in dallas_streets if s and s != 'UNNAMED STREET' and '/' not in s])\n",
        "print(f\"\\n✓ Dallas: {len(dallas_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Houston\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'Houston/original_data/COH_RoadCenterline_-7101768773205832403.csv', low_memory=False)\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['PREFIX', 'NAME', 'ST_TYPE', 'SUFFIX', 'FULL_NAME'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'FULL_NAME'\n",
        "houston_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "houston_streets = sorted([s for s in houston_streets if s])\n",
        "print(f\"\\n✓ Houston: {len(houston_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Jacksonville\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jacksonville file has no header - raw text with street name + zip\n",
        "with open(data_dir / 'Jacksonville/original_data/jacksonville street names - Sheet1.csv', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "print(f\"Total lines: {len(lines)} | No columns (raw text format)\")\n",
        "print(\"Sample rows:\")\n",
        "for line in lines[:5]:\n",
        "    print(f\"  {line.strip()}\")\n",
        "\n",
        "jacksonville_streets = []\n",
        "for line in lines:\n",
        "    line = line.strip().strip('\"')\n",
        "    parts = line.rsplit('  ', 1)\n",
        "    if parts:\n",
        "        street = parts[0].strip()\n",
        "        if street:\n",
        "            jacksonville_streets.append(street)\n",
        "\n",
        "jacksonville_streets = sorted(list(set(jacksonville_streets)))\n",
        "print(f\"\\n✓ Jacksonville: {len(jacksonville_streets)} unique street names (using 'raw text parsing')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Los Angeles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'LA/original_data/Street_Names_20251209.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['Street Name', 'Street Suffix', 'Street Suffix Direction', 'Official Street Name'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'Official Street Name'\n",
        "la_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "la_streets = sorted([s for s in la_streets if s])\n",
        "print(f\"\\n✓ Los Angeles: {len(la_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NYC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdb_path = data_dir / 'NYC/original_data/lion.gdb'\n",
        "df = gpd.read_file(gdb_path, layer='lion')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['Street', 'SAFStreetName', 'StreetCode'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'Street'\n",
        "nyc_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "nyc_streets = sorted([s for s in nyc_streets if s])\n",
        "print(f\"\\n✓ NYC: {len(nyc_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Philadelphia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'Philadelphia/original_data/Street_Centerline.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['pre_dir', 'st_name', 'st_type', 'suf_dir', 'streetlabe', 'stname'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'st_name'\n",
        "philly_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "philly_streets = sorted([s for s in philly_streets if s])\n",
        "print(f\"\\n✓ Philadelphia: {len(philly_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phoenix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'Phoenix/original_data/Street_Name_Labels.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['PREFIX', 'NAME', 'TYPE', 'SUFFIX', 'FULLNAME', 'ALIAS_NAME'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'FULLNAME'\n",
        "phoenix_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "phoenix_streets = sorted([s for s in phoenix_streets if s])\n",
        "print(f\"\\n✓ Phoenix: {len(phoenix_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## San Antonio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'SanAntonio/original_data/Streets_-5650981568868605735.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['MSAG_NAME', 'FROM_STREET', 'TO_STREET'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'MSAG_NAME'\n",
        "sanantonio_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "sanantonio_streets = sorted([s for s in sanantonio_streets if s])\n",
        "print(f\"\\n✓ San Antonio: {len(sanantonio_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## San Diego\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'SanDiego/original_data/roads_datasd.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['rd20full', 'rd30full', 'rd20pred', 'rd20name', 'rd20sfx'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'rd30full'\n",
        "sandiego_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "sandiego_streets = sorted([s for s in sandiego_streets if s and s != 'ALLEY'])\n",
        "print(f\"\\n✓ San Diego: {len(sandiego_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## San Jose\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'SanJose/original_data/Streets.csv')\n",
        "df['corename'] = df['FULLNAME'].apply(lambda x: \" \".join(x.split(' ')[:-1]))\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "#remove suffix\n",
        "show_cols = list(dict.fromkeys(['FULLNAME', 'STREETMASTERID', 'corename'] + name_cols))\n",
        "\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'corename'\n",
        "sanjose_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "sanjose_streets = sorted([s for s in sanjose_streets if s])\n",
        "print(f\"\\n✓ San Jose: {len(sanjose_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## San Francisco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir / 'SF/original_data/Street_Names_20251209.csv')\n",
        "print(f\"Shape: {df.shape} | Columns: {list(df.columns)}\")\n",
        "name_cols = [c for c in df.columns if 'name' in c.lower()]\n",
        "show_cols = list(dict.fromkeys(['FullStreetName', 'StreetName', 'StreetType', 'PostDirection'] + name_cols))\n",
        "display(df[show_cols].head())\n",
        "\n",
        "col_used = 'FullStreetName'\n",
        "sf_streets = df[col_used].dropna().str.strip().unique().tolist()\n",
        "sf_streets = sorted([s for s in sf_streets if s])\n",
        "print(f\"\\n✓ San Francisco: {len(sf_streets)} unique street names (using '{col_used}')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Combine All Street Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load USPS Street Suffix Abbreviations from CSV (source: Publication 28)\n",
        "# https://pe.usps.com/text/pub28/28apc_002.htm\n",
        "usps_suffixes_df = pd.read_csv(data_dir / 'usps_street_suffixes.csv')\n",
        "\n",
        "# Build set of all valid suffixes (primary names, common abbreviations, and USPS standard)\n",
        "SUFFIXES = set()\n",
        "for _, row in usps_suffixes_df.iterrows():\n",
        "    SUFFIXES.add(row['primary_name'].upper())\n",
        "    SUFFIXES.add(row['usps_abbrev'].upper())\n",
        "    for abbrev in str(row['common_abbrev']).split(';'):\n",
        "        if abbrev.strip():\n",
        "            SUFFIXES.add(abbrev.strip().upper())\n",
        "\n",
        "print(f\"Loaded {len(SUFFIXES)} unique street suffixes from USPS data\")\n",
        "\n",
        "# Directional indicators\n",
        "DIRECTIONS = {'N', 'S', 'E', 'W', 'NE', 'NW', 'SE', 'SW', \n",
        "              'NORTH', 'SOUTH', 'EAST', 'WEST', 'NORTHEAST', 'NORTHWEST', 'SOUTHEAST', 'SOUTHWEST'}\n",
        "\n",
        "# Pattern for numbered streets (1ST, 2ND, 3RD, 4TH, 42ND, 100TH, etc.)\n",
        "NUMBERED_PATTERN = re.compile(r'^\\d+(ST|ND|RD|TH)?$', re.IGNORECASE)\n",
        "\n",
        "def parse_street_name(street_name):\n",
        "    \"\"\"Parse a street name into components: suffix, direction, corename, is_numbered.\"\"\"\n",
        "    if not street_name:\n",
        "        return None, None, None, False\n",
        "    \n",
        "    parts = street_name.upper().split()\n",
        "    if not parts:\n",
        "        return None, None, None, False\n",
        "    \n",
        "    direction = None\n",
        "    suffix = None\n",
        "    core_parts = parts.copy()\n",
        "    \n",
        "    # Check for prefix direction\n",
        "    if parts[0] in DIRECTIONS:\n",
        "        direction = parts[0]\n",
        "        core_parts = core_parts[1:]\n",
        "    \n",
        "    # Check for suffix direction (at the end)\n",
        "    if core_parts and core_parts[-1] in DIRECTIONS:\n",
        "        if direction is None:\n",
        "            direction = core_parts[-1]\n",
        "        else:\n",
        "            direction = f\"{direction}-{core_parts[-1]}\"  # Both prefix and suffix direction\n",
        "        core_parts = core_parts[:-1]\n",
        "    \n",
        "    # Check for street type suffix\n",
        "    if core_parts and core_parts[-1] in SUFFIXES:\n",
        "        suffix = core_parts[-1]\n",
        "        core_parts = core_parts[:-1]\n",
        "    \n",
        "    # Core name is what's left\n",
        "    corename = ' '.join(core_parts) if core_parts else None\n",
        "    \n",
        "    # Check if numbered street\n",
        "    is_numbered = False\n",
        "    if corename:\n",
        "        core_first = corename.split()[0] if corename.split() else ''\n",
        "        is_numbered = bool(NUMBERED_PATTERN.match(core_first))\n",
        "    \n",
        "    return suffix, direction, corename, is_numbered\n",
        "\n",
        "# Combine all datasets\n",
        "all_streets = []\n",
        "\n",
        "datasets = {\n",
        "    'Chicago': chicago_streets,\n",
        "    'Dallas': dallas_streets,\n",
        "    'Houston': houston_streets,\n",
        "    'Jacksonville': jacksonville_streets,\n",
        "    'Los Angeles': la_streets,\n",
        "    'NYC': nyc_streets,\n",
        "    'Philadelphia': philly_streets,\n",
        "    'Phoenix': phoenix_streets,\n",
        "    'San Antonio': sanantonio_streets,\n",
        "    'San Diego': sandiego_streets,\n",
        "    'San Jose': sanjose_streets,\n",
        "    'San Francisco': sf_streets,\n",
        "}\n",
        "\n",
        "for city, streets in datasets.items():\n",
        "    for street in streets:\n",
        "        suffix, direction, corename, is_numbered = parse_street_name(street)\n",
        "        all_streets.append({\n",
        "            'street_name': street,\n",
        "            'corename': corename,\n",
        "            'suffix': suffix,\n",
        "            'direction': direction,\n",
        "            'is_numbered': is_numbered,\n",
        "            'city': city\n",
        "        })\n",
        "\n",
        "combined_df = pd.DataFrame(all_streets)\n",
        "combined_df.to_csv(\"combined_df.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# ============================================================================\n",
        "# FILTERING: Remove problematic street names\n",
        "# ============================================================================\n",
        "\n",
        "def is_valid_street_name(row):\n",
        "    \"\"\"Check if a street name should be included (returns True if valid).\"\"\"\n",
        "    street = str(row['street_name']).upper() if pd.notna(row['street_name']) else ''\n",
        "    corename = str(row['corename']).upper() if pd.notna(row['corename']) else ''\n",
        "    \n",
        "    # Exclude numbered streets\n",
        "    if row['is_numbered']:\n",
        "        return False\n",
        "    \n",
        "    # Exclude empty/NaN corenames\n",
        "    if not corename or corename == 'NAN' or corename.strip() == '':\n",
        "        return False\n",
        "    \n",
        "    # Exclude single-letter street names (A ST, B ST, C ST, etc.)\n",
        "    if len(corename.strip()) == 1:\n",
        "        return False\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Exclude highways, expressways, ramps, thruways\n",
        "    # -------------------------------------------------------------------------\n",
        "    highway_patterns = [\n",
        "        r'^I\\d+',           # Interstate highways (I10, I57, I95)\n",
        "        r'^US\\d+',          # US routes (US1, US101)\n",
        "        r'^SR\\d+',          # State routes (SR99)\n",
        "        r'^HWY\\s*\\d+',      # Highway numbers\n",
        "        r'^ROUTE\\s*\\d+',    # Route numbers\n",
        "    ]\n",
        "    for pattern in highway_patterns:\n",
        "        if re.match(pattern, street):\n",
        "            return False\n",
        "    \n",
        "    # Keywords that indicate highways/ramps/infrastructure (not real street names)\n",
        "    exclude_keywords = [\n",
        "        'EXPWY', 'EXPY', 'EXPRESSWAY',  # Expressways\n",
        "        'THRWY', 'THWY', 'THRUWAY', 'THROUGHWAY',  # Thruways\n",
        "        'FRWY', 'FREEWAY', 'FWY',  # Freeways\n",
        "        'RAMP',  # Ramps\n",
        "        'EXIT', ' ET ',  # Exit indicators\n",
        "        ' NB ', ' SB ', ' EB ', ' WB ',  # Directional traffic (northbound, etc.)\n",
        "        ' NB$', ' SB$', ' EB$', ' WB$',  # Directional at end\n",
        "        '^NB ', '^SB ', '^EB ', '^WB ',  # Directional at start\n",
        "        ' TO ',  # \"TO\" connector (e.g., \"NB TO SB\")\n",
        "        ' ON$', ' OFF$',  # On/off ramp indicators\n",
        "        ' ON ', ' OFF ',  # On/off in middle\n",
        "        '@',  # Highway interchange notation\n",
        "        'UNNAMED',  # Unnamed streets\n",
        "        'AIRTRAIN', 'SUBWAY', ' LINE$', ' LINE ',  # Transit lines\n",
        "        'CONNECTOR', 'INTERCHANGE',  # Infrastructure\n",
        "        '//',  # Double slash\n",
        "    ]\n",
        "    \n",
        "    for keyword in exclude_keywords:\n",
        "        if keyword.startswith('^'):\n",
        "            if re.match(keyword[1:], street):\n",
        "                return False\n",
        "        elif keyword.endswith('$'):\n",
        "            if re.search(keyword[:-1] + '$', street):\n",
        "                return False\n",
        "        elif keyword in street:\n",
        "            return False\n",
        "    \n",
        "    # Exclude streets with slashes (often combined routes/directions)\n",
        "    if '/' in street:\n",
        "        return False\n",
        "    \n",
        "    # Exclude streets ending in single letters like \"D\", \"G-H\" (unit/building designators)\n",
        "    # Match pattern: ends with space + single letter or letter-letter\n",
        "    if re.search(r'\\s+[A-Z]$', corename) or re.search(r'\\s+[A-Z]-[A-Z]$', corename):\n",
        "        return False\n",
        "    \n",
        "    # Exclude alphanumeric codes (like A143, 040, 8025B)\n",
        "    if re.search(r'^[A-Z]?\\d{2,}[A-Z]?$', corename):\n",
        "        return False\n",
        "    if re.search(r'\\d{3,}[A-Z]?$', street):  # Ends with 3+ digit number\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Apply the filter\n",
        "filtered_df = combined_df[combined_df.apply(is_valid_street_name, axis=1)].copy()\n",
        "\n",
        "print(f\"Original: {len(combined_df):,} streets\")\n",
        "print(f\"After filtering: {len(filtered_df):,} streets\")\n",
        "print(f\"Removed: {len(combined_df) - len(filtered_df):,} streets ({(len(combined_df) - len(filtered_df)) / len(combined_df) * 100:.1f}%)\")\n",
        "\n",
        "# Sample 100 per city from the filtered data\n",
        "sampled_df = filtered_df.groupby(\"city\").sample(n=100, random_state=589208)\n",
        "sampled_df.to_csv(\"street_names_100_per_city_random_state_589208.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"\\nSampled: {len(sampled_df)} streets (100 per city)\")\n",
        "print(f\"Unique corenames: {sampled_df['corename'].nunique()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prefixes = pd.read_csv(\"prefixes.csv\")\n",
        "\n",
        "# Create a cycling iterator for even distribution of prefixes\n",
        "prefix_cycle_idx = [0]  # Use list to allow mutation in closure\n",
        "\n",
        "def phrase_generator(x):\n",
        "    idx = prefix_cycle_idx[0] % len(prefixes)\n",
        "    prefix_cycle_idx[0] += 1\n",
        "    \n",
        "    row = prefixes.iloc[idx]\n",
        "    prefix = row['prefix']\n",
        "    question = row['question']\n",
        "    if question == \"yes\":\n",
        "        return prefix + \" \" + x + \"?\"\n",
        "    else:\n",
        "        return prefix + \" \" + x + \".\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample 30 per city for study\n",
        "sampled_df = filtered_df.groupby(\"city\").sample(n=30, random_state=9872).sample(frac=1).reset_index(drop=True)\n",
        "prefix_cycle_idx[0] = 0  # Reset counter before generating phrases\n",
        "sampled_df['phrase'] = sampled_df['corename'].apply(lambda x: f\"{phrase_generator(x)}\")\n",
        "sampled_df.to_csv(\"street_names_30_per_city_random_state_9872.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"\\nSampled: {len(sampled_df)} streets (30 per city) for study\")\n",
        "print(f\"Unique corenames: {sampled_df['corename'].nunique()}\")\n",
        "\n",
        "# Validate even distribution of prefixes\n",
        "prefix_counts = sampled_df['phrase'].apply(lambda p: next((row['prefix'] for _, row in prefixes.iterrows() if p.startswith(row['prefix'])), None)).value_counts()\n",
        "print(f\"\\nPrefix distribution (expected {len(sampled_df) // len(prefixes)} each):\")\n",
        "print(prefix_counts.to_string())\n",
        "assert prefix_counts.nunique() == 1, f\"Uneven distribution! Counts: {prefix_counts.unique()}\"\n",
        "print(\"✓ All prefixes used evenly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
