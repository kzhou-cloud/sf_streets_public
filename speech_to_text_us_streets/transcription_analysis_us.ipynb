{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c6d1fc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Import all required libraries\n",
        "# ============================================================================\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import string\n",
        "import sys\n",
        "import textwrap\n",
        "import unicodedata\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "from scipy import stats\n",
        "\n",
        "from utils import (\n",
        "    normalize_text,\n",
        "    process_primary_language,\n",
        "    categorize_language,\n",
        "    group_language_by_family,\n",
        "    check_english_only,\n",
        "    check_multilingual,\n",
        "    check_not_english,\n",
        "    read_transcription_data,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "matplotlib.rcParams['font.family'] = 'Arial'\n",
        "matplotlib.rcParams['font.size'] = 24\n",
        "\n",
        "pd.set_option('display.max_rows', 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affa8f87",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all model families together\n",
        "MODEL_FAMILY = \"whisper\"\n",
        "\n",
        "# Model to model family mapping\n",
        "MODEL_TO_FAMILY = {\n",
        "    # Whisper models\n",
        "    'tiny': 'whisper',\n",
        "    'base': 'whisper',\n",
        "    'small': 'whisper', \n",
        "    'medium': 'whisper',\n",
        "    'large': 'whisper',\n",
        "    'turbo': 'whisper',\n",
        "}\n",
        "\n",
        "# Define model sizes (in millions of parameters) for use in visualizations\n",
        "model_sizes = {\n",
        "    'tiny': 39,\n",
        "    'base': 74,\n",
        "    'small': 244,\n",
        "    'medium': 769,\n",
        "    'large': 1550,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8e6eaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def read_transcription_data(MODEL_FAMILY, meta_demographics_file, verbose=True):\n",
        "    \"\"\"\n",
        "    Load transcription data and join with demographics from meta file.\n",
        "    \n",
        "    Args:\n",
        "        MODEL_FAMILY: Model family to load ('whisper', 'all', etc.)\n",
        "        meta_demographics_file: Path to CSV file with all demographic data\n",
        "        verbose: Print loading progress\n",
        "    \"\"\"\n",
        "    # Load all TSV files from the selected model family\n",
        "    if MODEL_FAMILY == 'all':\n",
        "        tsv_files = glob.glob(\"transcriptions/*/*.tsv\")\n",
        "    else:\n",
        "        tsv_files = glob.glob(f\"transcriptions/{MODEL_FAMILY}/*.tsv\")\n",
        "\n",
        "    if len(tsv_files) == 0:\n",
        "        raise ValueError(f\"No TSV files found for MODEL_FAMILY='{MODEL_FAMILY}'\")\n",
        "    elif len(tsv_files) == 1:\n",
        "        data = pd.read_csv(tsv_files[0], sep='\\t')\n",
        "    else:\n",
        "        data = pd.concat([pd.read_csv(f, sep='\\t') for f in tsv_files], ignore_index=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Loaded {len(data)} rows from {len(tsv_files)} file(s) ({MODEL_FAMILY})\")\n",
        "\n",
        "    data['transcription_og'] = data['transcription']\n",
        "    if 'prompt' in data.columns:\n",
        "        data['prompt'] = data['prompt'].fillna(\"No prompt\")\n",
        "    else:\n",
        "        data['prompt'] = \"No prompt\"\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Load demographics from meta file\n",
        "    demo = pd.read_csv(meta_demographics_file)\n",
        "    demo = demo[demo['Status'] == 'APPROVED']\n",
        "    if verbose:\n",
        "        print(f\"Loaded {len(demo)} approved records from meta demographics file\")\n",
        "\n",
        "    demo['Primary language'] = demo['Primary language'].apply(process_primary_language)\n",
        "    \n",
        "    # Join transcription data with demographics\n",
        "    data = data.set_index(\"participant_id\").join(demo.set_index(\"Participant id\"), how='left').reset_index()\n",
        "    data = data.rename(columns={'index': 'participant_id'})\n",
        "    \n",
        "    data = data.dropna(subset=['Primary language'])\n",
        "\n",
        "    data['english_only'] = data['Primary language'].apply(check_english_only)\n",
        "    data['multilingual'] = data['Primary language'].apply(check_multilingual)\n",
        "    data['not_english'] = data['Primary language'].apply(check_not_english)\n",
        "\n",
        "    data['answer'] = data['original_text'].apply(lambda x: str(x).split(\":\")[-1].replace('\"', \"\").lower())\n",
        "    data['answer'] = data['answer'].apply(lambda x: normalize_text(x))\n",
        "    data['transcription'] = data['transcription'].apply(lambda x: normalize_text(x))\n",
        "\n",
        "    data['levenshtein_distance'] = data.apply(lambda row: levenshtein_distance(row['answer'], row['transcription']), axis=1)\n",
        "    data['is_correct'] = data['levenshtein_distance'] == 0\n",
        "\n",
        "    data['Age'] = data['Age'].astype(int)\n",
        "    data['age_decade'] = data['Age'] // 10\n",
        "\n",
        "    # prefer not to say grouped as 1\n",
        "    data['Sex'] = data['Sex'].apply(lambda x: 0 if x == 'Male' else 1)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31870028",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "meta_demographics_file = \"demographic_data/grandfather_task_demographics.csv\"\n",
        "\n",
        "data = read_transcription_data(MODEL_FAMILY, meta_demographics_file=meta_demographics_file)\n",
        "\n",
        "#getting rid of some inconsistent whitespace --- same prompt was passed in\n",
        "data['prompt'] = data['prompt'].str.replace('\\xa0', ' ', regex=False)\n",
        "\n",
        "street_origin = pd.read_csv(\"../street_names.tsv\")\n",
        "street_origin['name'] = street_origin['name'].str.lower()\n",
        "\n",
        "\n",
        "data = data.set_index(\"answer\").join(street_origin.set_index(\"name\"), how='left').reset_index()\n",
        "data.columns = ['answer', 'participant_id', 'index', 'model', 'original_text',\n",
        "       'transcription', 'transcription_og', 'prompt', 'Submission id',\n",
        "       'Status', 'Custom study tncs accepted at', 'Started at', 'Completed at',\n",
        "       'Reviewed at', 'Archived at', 'Time taken', 'Completion code',\n",
        "       'Total approvals', 'Primary language', 'Age', 'Sex',\n",
        "       'Ethnicity simplified', 'Country of birth', 'Country of residence',\n",
        "       'Nationality', 'Language', 'Student status', 'Employment status',\n",
        "       'english_only', 'multilingual', 'not_english', 'levenshtein_distance',\n",
        "       'is_correct', 'age_decade', 'origin']\n",
        "data['correct'] = data['transcription'] == data['answer']\n",
        "data['language_family'] = data['Primary language'].apply(group_language_by_family)\n",
        "\n",
        "# Add model_family column based on model name\n",
        "data['model_family'] = data['model'].map(MODEL_TO_FAMILY)\n",
        "\n",
        "data['language_group'] = data['Primary language'].apply(categorize_language)\n",
        "data['is_correct'] = data['is_correct'].astype(float)\n",
        "\n",
        "data['model'] = data['model'].replace(\"whisper_large_synthetic_16633_20251216_160230\", \"large-finetuned\")\n",
        "print(f\"\\nModel families loaded: {data['model_family'].unique().tolist()}\")\n",
        "print(f\"Models per family:\\n{data.groupby('model_family')['model'].unique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08353522",
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO some data cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad85a4da",
      "metadata": {},
      "outputs": [],
      "source": [
        "data[data['model'] == 'base'].groupby(\"participant_id\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01caf7b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "participant_data = data.groupby(\"participant_id\").sample(n=1)\n",
        "display(participant_data.groupby(\"Sex\").count())\n",
        "display(participant_data.groupby(\"Sex\").count()/93)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb389caa",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(participant_data.groupby(\"age_decade\").count())\n",
        "display((participant_data.groupby(\"age_decade\").count()/93).round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632303d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(participant_data.groupby(\"language_group\").count())\n",
        "display(participant_data.groupby(\"language_group\").count()/93)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e7435b",
      "metadata": {},
      "outputs": [],
      "source": [
        "value = str([x.replace(\"'\", \"\").replace(\"'\", \"\")  for x in participant_data[\"Primary language\"].unique()])\n",
        "values = value.split(\",\")\n",
        "values = [x.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\") for x in values]\n",
        "\n",
        "print(set(values))\n",
        "print(len(set(values)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de49deb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Bootstrap sampling function with confidence intervals\n",
        "def bootstrap_accuracy(data, n_bootstrap=10000, confidence_level=0.95):\n",
        "    n = len(data)\n",
        "    if n == 0:\n",
        "        return {'mean': np.nan, 'lower': np.nan, 'upper': np.nan, 'std': np.nan}\n",
        "    \n",
        "    # Store bootstrap accuracies\n",
        "    bootstrap_accuracies = np.zeros(n_bootstrap)\n",
        "    \n",
        "    # Perform bootstrap sampling\n",
        "    for i in range(n_bootstrap):\n",
        "        # Sample with replacement\n",
        "        bootstrap_sample = data.sample(n=n, replace=True)\n",
        "        bootstrap_accuracies[i] = bootstrap_sample['is_correct'].mean()\n",
        "    \n",
        "    # Calculate confidence interval\n",
        "    alpha = 1 - confidence_level\n",
        "    lower_percentile = (alpha / 2) * 100\n",
        "    upper_percentile = (1 - alpha / 2) * 100\n",
        "    \n",
        "    ci_lower = np.percentile(bootstrap_accuracies, lower_percentile)\n",
        "    ci_upper = np.percentile(bootstrap_accuracies, upper_percentile)\n",
        "    \n",
        "    return {\n",
        "        'mean': data['is_correct'].mean(),\n",
        "        'lower': ci_lower,\n",
        "        'upper': ci_upper,\n",
        "        'std': np.std(bootstrap_accuracies)\n",
        "    }\n",
        "\n",
        "def calculate_bootstrap_by_model_group(data, n_bootstrap=10000):\n",
        "    results = []\n",
        "    \n",
        "    for (model, language_group), group_data in data.groupby(['model', 'language_group'], observed=True):\n",
        "        boot_result = bootstrap_accuracy(group_data, n_bootstrap=n_bootstrap)\n",
        "        results.append({\n",
        "            'model': model,\n",
        "            'language_group': language_group,\n",
        "            'mean': boot_result['mean'],\n",
        "            'lower': boot_result['lower'],\n",
        "            'upper': boot_result['upper'],\n",
        "            'std': boot_result['std']\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df['model_size'] = results_df['model'].map(model_sizes)\n",
        "    \n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c005cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "data.groupby(['prompt', 'language_group']).mean(numeric_only=True)[['is_correct']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fdba21c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization with bootstrap confidence bands\n",
        "\n",
        "def visualize_data_with_confidence_bands(data, ax, prompt, n_bootstrap=10000, colors=None):\n",
        "    \"\"\"\n",
        "    Visualize accuracy data with 95% confidence bands using bootstrap sampling.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data to visualize\n",
        "    ax : matplotlib axis\n",
        "        Axis to plot on\n",
        "    prompt : str\n",
        "        Prompt text for the title\n",
        "    n_bootstrap : int\n",
        "        Number of bootstrap samples (default: 10000)\n",
        "    colors : dict\n",
        "        Colors for each language group (default: None, uses default colors)\n",
        "    \"\"\"\n",
        "    # Filter out DATA_EXPIRED language\n",
        "    data_filtered = data.copy()\n",
        "    \n",
        "    # Add language group\n",
        "    data_filtered['language_group'] = data_filtered['Primary language'].apply(categorize_language)\n",
        "    \n",
        "    # Calculate bootstrap confidence intervals\n",
        "    print(f\"Calculating bootstrap confidence intervals for: {prompt[:50]}...\")\n",
        "    bootstrap_results = calculate_bootstrap_by_model_group(data_filtered, n_bootstrap=n_bootstrap)\n",
        "    \n",
        "    # Define colors for the three groups (darkest = Non-English, lightest = English only)\n",
        "    if colors is None:\n",
        "        colors = {'English only': '#59A14F', 'Multilingual (English)': '#76B7B2', 'Non-English': '#E15759'}\n",
        "    \n",
        "    # Filter to only models with sizes in model_sizes\n",
        "    bootstrap_results = bootstrap_results.dropna(subset=['model_size'])\n",
        "    \n",
        "    # Use log scale for x-axis\n",
        "    ax.set_xscale('log')\n",
        "    \n",
        "    # Plot each language group as a separate line with confidence bands\n",
        "    for language_group in ['English only', 'Multilingual (English)', 'Non-English']:\n",
        "        group_data = bootstrap_results[bootstrap_results['language_group'] == language_group]\n",
        "        # Sort by model size and reset index to ensure proper alignment\n",
        "        group_data = group_data.sort_values('model_size').reset_index(drop=True)\n",
        "        \n",
        "        if len(group_data) > 0:\n",
        "            # Use model sizes for x positions\n",
        "            x = group_data['model_size'].values\n",
        "            y_mean = group_data['mean'].values\n",
        "            y_lower = group_data['lower'].values\n",
        "            y_upper = group_data['upper'].values\n",
        "            \n",
        "            # Plot confidence band (shaded area)\n",
        "            ax.fill_between(x, y_lower, y_upper, alpha=0.2, color=colors[language_group])\n",
        "            \n",
        "            # Plot line (lighter)\n",
        "            ax.plot(x, y_mean, linewidth=2, alpha=0.3, color=colors[language_group])\n",
        "            \n",
        "            # Plot markers with full opacity\n",
        "            ax.plot(x, y_mean, marker='o', linewidth=0, markersize=8, \n",
        "                    color=colors[language_group], label=language_group)\n",
        "    \n",
        "    \n",
        "    ax.set_xlabel('Model Size (Millions of Parameters)', fontsize=12)\n",
        "    ax.set_ylabel('Accuracy', fontsize=12)\n",
        "    \n",
        "    # Wrap title to multiple lines if needed\n",
        "    wrapped_title = '\\n'.join(textwrap.wrap(prompt[:80], width=50))\n",
        "    ax.set_title(wrapped_title, fontsize=14)\n",
        "    \n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.set_xlim(25, 20000)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1a11fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create plot with 3 subplots and confidence bands\n",
        "print(\"Creating visualization with bootstrap confidence bands...\")\n",
        "print(\"Note: This may take a few minutes due to 10,000 bootstrap samples per group.\\n\")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(figsize=(15, 6), nrows=1, ncols=3)\n",
        "\n",
        "\n",
        "# Get unique prompts, filtering out NaN values, and sort\n",
        "unique_prompts = sorted([p for p in data['prompt'].unique() if pd.notna(p)])\n",
        "for n, prompt in enumerate(unique_prompts):\n",
        "    visualize_data_with_confidence_bands(data[data['prompt'] == prompt], axes[n], prompt, n_bootstrap=10000)\n",
        "\n",
        "# Update axis labels to match style\n",
        "for ax in axes:\n",
        "    ax.set_xlabel('Model Size (Millions of Parameters)', fontsize=14)\n",
        "    ax.set_ylabel('Average Accuracy', fontsize=14)\n",
        "\n",
        "# Create a single shared legend below all subplots\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, title='Language Group', \n",
        "           bbox_to_anchor=(0.5, -0.02), loc='upper center', ncol=3, \n",
        "           frameon=True, fontsize=12, title_fontsize=12)\n",
        "\n",
        "fig.suptitle('Transcription Accuracy by Prompt Type w/ 95% CI', fontsize=16, y=1.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save to figures folder as high-quality PDF\n",
        "output_dir = \"figures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get model names from the data\n",
        "model_names = \"_\".join(sorted(data['model'].unique()))\n",
        "output_filename = f\"all_models_accuracy_{model_names}.pdf\"\n",
        "output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "plt.savefig(f'figures/accuracy_by_figure_2.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"\\n✓ Saved high-quality PDF to: {output_path}\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization complete with 95% bootstrap confidence intervals!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbd8830",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single graph with three lines - one for each prompt type\n",
        "# Shows average performance across all language groups\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Get unique prompts\n",
        "unique_prompts = sorted([p for p in data['prompt'].unique() if pd.notna(p)])\n",
        "\n",
        "# Define colors for each prompt\n",
        "prompt_colors = ['#2E86AB', '#F18F01', '#A23B72']\n",
        "prompt_labels = ['No prompt', 'Prompt - The user is going to give you their location via an address ', 'The user is going to give you their location via one of the following addresses:...']\n",
        "\n",
        "ax.set_xscale('log')\n",
        "\n",
        "for idx, prompt in enumerate(unique_prompts):\n",
        "    prompt_data = data[data['prompt'] == prompt].copy()\n",
        "    \n",
        "    # Calculate average accuracy by model (across all language groups)\n",
        "    model_accuracy = prompt_data.groupby('model').agg({\n",
        "        'is_correct': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Add model sizes\n",
        "    model_accuracy['model_size'] = model_accuracy['model'].map(model_sizes)\n",
        "    model_accuracy = model_accuracy.dropna(subset=['model_size'])\n",
        "    model_accuracy = model_accuracy.sort_values('model_size')\n",
        "    \n",
        "    # Calculate bootstrap CI for each model\n",
        "    ci_results = []\n",
        "    for model in model_accuracy['model'].unique():\n",
        "        model_data = prompt_data[prompt_data['model'] == model]\n",
        "        if len(model_data) > 0:\n",
        "            boot_result = bootstrap_accuracy(model_data, n_bootstrap=10000)\n",
        "            ci_results.append({\n",
        "                'model': model,\n",
        "                'model_size': model_sizes.get(model, np.nan),\n",
        "                'mean': boot_result['mean'],\n",
        "                'lower': boot_result['lower'],\n",
        "                'upper': boot_result['upper']\n",
        "            })\n",
        "    \n",
        "    ci_df = pd.DataFrame(ci_results).dropna(subset=['model_size']).sort_values('model_size')\n",
        "    \n",
        "    if len(ci_df) > 0:\n",
        "        x = ci_df['model_size'].values\n",
        "        y_mean = ci_df['mean'].values\n",
        "        y_lower = ci_df['lower'].values\n",
        "        y_upper = ci_df['upper'].values\n",
        "        \n",
        "        # Plot confidence band\n",
        "        ax.fill_between(x, y_lower, y_upper, alpha=0.2, color=prompt_colors[idx])\n",
        "        \n",
        "        # Plot line\n",
        "        ax.plot(x, y_mean, marker='o', linewidth=2, markersize=8, \n",
        "                color=prompt_colors[idx], label=prompt_labels[idx] if idx < len(prompt_labels) else prompt[:30])\n",
        "        \n",
        "        # Add model labels (only for first prompt to avoid clutter)\n",
        "        if idx == 0:\n",
        "            label_y = 0.98  # Fixed y-coordinate for all labels\n",
        "            for i, row in ci_df.iterrows():\n",
        "                # Draw vertical line from label to data point\n",
        "                ax.plot([row['model_size'], row['model_size']], [row['mean'], label_y - 0.07], \n",
        "                       color='gray', linewidth=0.5, linestyle='-', alpha=0.5)\n",
        "                # Add label at fixed y position\n",
        "                ax.text(row['model_size'], label_y - 0.07, row['model'], \n",
        "                       ha='center', va='bottom', fontsize=14, rotation=30)\n",
        "\n",
        "# Add dashed horizontal lines for average accuracy of each prompt\n",
        "for idx, prompt in enumerate(unique_prompts):\n",
        "    prompt_data = data[data['prompt'] == prompt]\n",
        "    avg_accuracy = prompt_data['is_correct'].mean()\n",
        "    ax.axhline(y=avg_accuracy, color=prompt_colors[idx], linestyle='--', linewidth=.7, alpha=0.7)\n",
        "    # Add label on right side\n",
        "    ax.text(22000, avg_accuracy, f'{avg_accuracy:.2f}', \n",
        "            color=prompt_colors[idx], fontsize=14, va='center')\n",
        "\n",
        "ax.set_xlabel('Model Size in Millions of Parameters (Log Scale)', fontsize=14)\n",
        "ax.set_ylabel('Average Accuracy', fontsize=14)\n",
        "ax.set_title('Transcription Accuracy by Prompt Type w/ 95% CI', \n",
        "             fontsize=16)\n",
        "ax.set_ylim(0, 1.0)\n",
        "ax.set_xlim(25, 20000)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(title='Prompt Type', fontsize=12, title_fontsize=12, loc='lower right')\n",
        "\n",
        "plt.savefig(f'figures/overall_accuracies_figure_1.png', dpi=150, bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d7f2a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate grouped means\n",
        "grouped_means = data[data['prompt']=='No prompt'][data['model']!='large-finetuned'].groupby(['model', 'language_group']).mean(numeric_only=True).sort_values('is_correct', ascending=False)[['is_correct']]\n",
        "\n",
        "# Calculate confidence intervals for each group using bootstrap_accuracy function from Cell 10\n",
        "ci_results = []\n",
        "for (model, language_group), group_data in data[data['prompt']=='No prompt'][data['model']!='large-finetuned'].groupby(['model', 'language_group'], observed=True):\n",
        "    boot_result = bootstrap_accuracy(group_data, n_bootstrap=10)\n",
        "    ci_results.append({\n",
        "        'model': model,\n",
        "        'language_group': language_group,\n",
        "        'mean': boot_result['mean'],\n",
        "        'ci_lower': boot_result['lower'],\n",
        "        'ci_upper': boot_result['upper'],\n",
        "        'error_lower': boot_result['mean'] - boot_result['lower'],\n",
        "        'error_upper': boot_result['upper'] - boot_result['mean'],\n",
        "        'n': len(group_data),\n",
        "        'std': boot_result['std']\n",
        "    })\n",
        "\n",
        "ci_df = pd.DataFrame(ci_results)\n",
        "\n",
        "# Add model_family to ci_df\n",
        "ci_df['model_family'] = ci_df['model'].map(MODEL_TO_FAMILY)\n",
        "ci_df = ci_df.sort_values(by='mean', ascending=True)\n",
        "\n",
        "# Get unique model families (excluding NaN)\n",
        "model_families = [f for f in ci_df['model_family'].unique() if pd.notna(f)]\n",
        "\n",
        "# Define colors for language groups\n",
        "colors = {\n",
        "    'English only': '#2E86AB', \n",
        "    'Multilingual (English)': '#F18F01', \n",
        "    'Non-English': '#A23B72'\n",
        "}\n",
        "language_groups = ['English only', 'Multilingual (English)', 'Non-English']\n",
        "\n",
        "# Create single figure with horizontal subplots scaled by number of models\n",
        "n_families = len(model_families)\n",
        "\n",
        "# Calculate width ratios based on number of models in each family\n",
        "width_ratios = [len(ci_df[ci_df['model_family'] == f]['model'].unique()) for f in model_families]\n",
        "total_models = sum(width_ratios)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_families, figsize=(1.2 * total_models, 8), sharey=True,\n",
        "                         gridspec_kw={'width_ratios': width_ratios})\n",
        "if n_families == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "width = 0.25\n",
        "offsets = np.array([-width, 0, width])\n",
        "\n",
        "for idx, family in enumerate(model_families):\n",
        "    ax = axes[idx]\n",
        "    family_df = ci_df[ci_df['model_family'] == family]\n",
        "    models = family_df['model'].unique()\n",
        "    \n",
        "    # Set up bar positions\n",
        "    n_models = len(models)\n",
        "    x = np.arange(n_models)\n",
        "    \n",
        "    # Plot bars for each language group\n",
        "    for i, lang_group in enumerate(language_groups):\n",
        "        group_data = family_df[family_df['language_group'] == lang_group].copy()\n",
        "        group_data = group_data.set_index('model').reindex(models).reset_index()\n",
        "        \n",
        "        bars = ax.bar(x + offsets[i], \n",
        "                      group_data['mean'], \n",
        "                      width, \n",
        "                      label=lang_group,\n",
        "                      color=colors[lang_group],\n",
        "                      alpha=0.8,\n",
        "                      edgecolor='white',\n",
        "                      linewidth=1.5)\n",
        "        \n",
        "        \n",
        "        # Only plot error bars for non-NaN values\n",
        "        valid_mask = ~group_data['mean'].isna()\n",
        "        if valid_mask.any():\n",
        "            ax.errorbar(x[valid_mask] + offsets[i], \n",
        "                        group_data.loc[valid_mask, 'mean'],\n",
        "                        yerr=[group_data.loc[valid_mask, 'error_lower'].fillna(0), \n",
        "                              group_data.loc[valid_mask, 'error_upper'].fillna(0)],\n",
        "                        fmt='none',\n",
        "                        ecolor='black',\n",
        "                        elinewidth=0.7,\n",
        "                        capsize=4,\n",
        "                        capthick=0.7,\n",
        "                        alpha=0.7)\n",
        "    \n",
        "    \n",
        "    ax.set_title(f'{family.upper()}', fontsize=16)\n",
        "    # ax.set_xlabel('Model', fontsize=14)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models, fontsize=12, rotation=45, ha='right')\n",
        "    \n",
        "\n",
        "    ax.set_xlim(-0.5, n_models - 0.5)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "\n",
        "# Only add y-label to first subplot\n",
        "axes[0].set_ylabel('Accuracy (Mean ± 95% CI)', fontsize=14)\n",
        "\n",
        "# Add shared legend\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "leg = fig.legend(handles, labels, title='Primary Language', loc='upper center', \n",
        "           bbox_to_anchor=(0.5, 0.97), ncol=3, fontsize=11, title_fontsize=11,\n",
        "           columnspacing=2, handletextpad=0.8, frameon=True, fancybox=False,\n",
        "           edgecolor='gray', facecolor='white', framealpha=1.0)\n",
        "leg.get_frame().set_linewidth(0.5)\n",
        "\n",
        "fig.suptitle('Transcription Accuracy by Model Family and Language Group\\n(with 95% Bootstrap Confidence Intervals)', \n",
        "             fontsize=14, y=1.02)\n",
        "plt.savefig(f'figures/potential_figure_2.png', dpi=150, bbox_inches='tight')\n",
        "plt.subplots_adjust(wspace=0.15, left=0.06, right=0.98, bottom=0.18, top=0.85)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d7f2a2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96fe12e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for heatmap\n",
        "grouped_data = data[data['prompt']=='No prompt'].groupby([\"answer\", \"Primary language\"]).mean(numeric_only=True)[['is_correct']]\n",
        "\n",
        "# Pivot the data for heatmap (Primary language as rows, answer as columns)\n",
        "heatmap_data = grouped_data.reset_index().pivot(index='Primary language', columns='answer', values='is_correct')\n",
        "\n",
        "# Sort y-axis by average accuracy across all streets (descending)\n",
        "heatmap_data['_avg_row'] = heatmap_data.mean(axis=1)\n",
        "heatmap_data = heatmap_data.sort_values('_avg_row', ascending=False)\n",
        "heatmap_data = heatmap_data.drop('_avg_row', axis=1)\n",
        "\n",
        "# Sort x-axis by average accuracy across all languages (descending)\n",
        "col_averages = heatmap_data.mean(axis=0).sort_values(ascending=False)\n",
        "heatmap_data = heatmap_data[col_averages.index]\n",
        "\n",
        "# Create the heatmap\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "sns.heatmap(heatmap_data, annot=False, cmap='RdYlGn', center=0.5, \n",
        "            vmin=0, vmax=1, cbar_kws={'label': 'Accuracy'}, \n",
        "            linewidths=0.5, linecolor='gray', ax=ax)\n",
        "ax.set_title('Transcription Accuracy by Street Name and Language', fontsize=16, pad=15)\n",
        "ax.set_xlabel('')\n",
        "ax.set_ylabel('Language', fontsize=12)\n",
        "# Clean up y-tick labels: remove \"English\" unless it's the only language\n",
        "ytick_labels = [label.get_text() for label in ax.get_yticklabels()]\n",
        "cleaned_labels = []\n",
        "for label in ytick_labels:\n",
        "    if label.lower().strip() == 'english':\n",
        "        cleaned_labels.append(label)\n",
        "    else:\n",
        "        # Remove \"English, \" or \", English\" from the label\n",
        "        cleaned = label.replace('English, ', '').replace(', English', '')\n",
        "        cleaned_labels.append(cleaned)\n",
        "ax.set_yticklabels(cleaned_labels, fontsize=10)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
        "\n",
        "plt.subplots_adjust(left=0.15, right=0.95, top=0.92, bottom=0.25)\n",
        "plt.savefig(f'figures/all_models_accuracy_by_street_language.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21259fe3",
      "metadata": {},
      "outputs": [],
      "source": [
        "lep_data = pd.read_csv(\"population_by_language.tsv\", sep=\"\\t\")\n",
        "lep_data = lep_data[['Language', 'LEP_Population']] \n",
        "lep_data['LEP_Population'] = lep_data['LEP_Population'].astype(int)\n",
        "lep_data['LEP_Population_Percent'] = lep_data['LEP_Population'] / 151388\n",
        "lep_data = lep_data.sort_values('LEP_Population_Percent', ascending=False)\n",
        "\n",
        "# Create visualization\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Create bar chart\n",
        "bars = ax.barh(lep_data['Language'], lep_data['LEP_Population_Percent'], color='steelblue', alpha=0.8)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, val) in enumerate(zip(bars, lep_data['LEP_Population_Percent'])):\n",
        "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, \n",
        "            f'{val:.1%}', va='center', fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Proportion of LEP Population', fontsize=12)\n",
        "ax.set_ylabel('Language', fontsize=12)\n",
        "ax.set_title('Limited English Proficiency (LEP) Population by Language\\n(San Francisco)', \n",
        "             fontsize=14)\n",
        "ax.set_xlim(0, max(lep_data['LEP_Population_Percent']) * 1.15)\n",
        "\n",
        "plt.savefig(f'figures/LEP_SF.png', dpi=150, bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23095b2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# OLS Regression: Demographic Features Predicting Transcription Accuracy\n",
        "# ============================================================================\n",
        "\n",
        "# Prepare the data for regression\n",
        "regression_data = data.copy()\n",
        "\n",
        "# Create dummy variables for categorical features\n",
        "# Sex: create dummy (Female=1, Male=0 as reference)\n",
        "regression_data['is_female'] = (regression_data['Sex'] == 'Female').astype(float)\n",
        "\n",
        "# Language family dummies (Germanic as reference category since English is most common)\n",
        "language_family_dummies = pd.get_dummies(regression_data['language_family'], prefix='lang_family', drop_first=True, dtype=float)\n",
        "regression_data = pd.concat([regression_data, language_family_dummies], axis=1)\n",
        "\n",
        "# Convert boolean columns to float\n",
        "regression_data['multilingual'] = regression_data['multilingual'].astype(float)\n",
        "regression_data['not_english'] = regression_data['not_english'].astype(float)\n",
        "\n",
        "# Create age decade buckets (use existing age_decade column or create from Age)\n",
        "if 'age_decade' not in regression_data.columns:\n",
        "    regression_data['age_decade'] = (regression_data['Age'] // 10 * 10).astype(int)\n",
        "\n",
        "# Age decade dummies (youngest decade as reference)\n",
        "age_decade_dummies = pd.get_dummies(regression_data['age_decade'], prefix='age', drop_first=True, dtype=float)\n",
        "regression_data = pd.concat([regression_data, age_decade_dummies], axis=1)\n",
        "\n",
        "# Define independent variables (demographic features)\n",
        "demographic_features = ['is_female', 'multilingual', 'not_english']\n",
        "\n",
        "# Add age decade dummies (excluding reference category)\n",
        "age_decade_cols = [col for col in regression_data.columns if col.startswith('age_')]\n",
        "demographic_features.extend(age_decade_cols)\n",
        "\n",
        "# Add language family dummies (excluding reference category)\n",
        "lang_family_cols = [col for col in regression_data.columns if col.startswith('lang_family_')]\n",
        "demographic_features.extend(lang_family_cols)\n",
        "\n",
        "# Aggregate by participant: mean accuracy per participant, keep demographic features\n",
        "participant_data = regression_data.groupby('participant_id').agg({\n",
        "    'is_correct': 'mean',  # Mean accuracy across all street names\n",
        "    **{feat: 'first' for feat in demographic_features}  # Demographics are constant per participant\n",
        "}).reset_index()\n",
        "\n",
        "print(f\"Aggregated to {len(participant_data)} participants (from {len(regression_data)} observations)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Test each feature independently for variance explained\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Univariate Tests: Each Feature Tested Independently\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare y for univariate tests\n",
        "y_uni = participant_data['is_correct'].values.astype(float)\n",
        "valid_y = ~np.isnan(y_uni)\n",
        "\n",
        "# Define feature groups for testing\n",
        "feature_groups = {\n",
        "    'Sex (is_female)': ['is_female'],\n",
        "    'Multilingual': ['multilingual'],\n",
        "    'Non-English Speaker': ['not_english'],\n",
        "    'Age Decade': [col for col in participant_data.columns if col.startswith('age_')],\n",
        "    'Language Family': [col for col in participant_data.columns if col.startswith('lang_family_')]\n",
        "}\n",
        "\n",
        "univariate_results = []\n",
        "\n",
        "for group_name, features in feature_groups.items():\n",
        "    if not features or not all(f in participant_data.columns for f in features):\n",
        "        continue\n",
        "    \n",
        "    X_uni = participant_data[features].values.astype(float)\n",
        "    valid_idx = valid_y & ~np.isnan(X_uni).any(axis=1)\n",
        "    X_valid = X_uni[valid_idx]\n",
        "    y_valid = y_uni[valid_idx]\n",
        "    \n",
        "    # Add constant\n",
        "    X_valid = sm.add_constant(X_valid)\n",
        "    \n",
        "    # Fit univariate model\n",
        "    model = sm.OLS(y_valid, X_valid).fit()\n",
        "    \n",
        "    univariate_results.append({\n",
        "        'Feature': group_name,\n",
        "        'N Features': len(features),\n",
        "        'R²': model.rsquared,\n",
        "        'Adj. R²': model.rsquared_adj,\n",
        "        'F-statistic': model.fvalue,\n",
        "        'F p-value': model.f_pvalue,\n",
        "        'N': len(y_valid)\n",
        "    })\n",
        "\n",
        "# Display results as table\n",
        "univariate_df = pd.DataFrame(univariate_results)\n",
        "univariate_df['Significant'] = univariate_df['F p-value'].apply(lambda p: '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else '')\n",
        "print(\"\\n\")\n",
        "print(univariate_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
        "print(\"\\nSignificance: * p<0.05, ** p<0.01, *** p<0.001\")\n",
        "\n",
        "# ============================================================================\n",
        "# Full Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Full Model: All Demographic Features\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare X and y as numpy arrays (from participant-level data)\n",
        "X = participant_data[demographic_features].values.astype(float)\n",
        "y = participant_data['is_correct'].values.astype(float)\n",
        "feature_names = ['const'] + demographic_features\n",
        "\n",
        "# Drop rows with NaN\n",
        "valid_idx = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
        "X = X[valid_idx]\n",
        "y = y[valid_idx]\n",
        "\n",
        "# Add constant for intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit OLS model\n",
        "ols_model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print regression summary\n",
        "print(\"=\" * 80)\n",
        "print(\"OLS Regression: Demographic Features Predicting Transcription Accuracy\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nDependent Variable: is_correct (mean accuracy per participant)\")\n",
        "print(f\"Number of observations: {len(y)}\")\n",
        "print(f\"\\nReference categories:\")\n",
        "print(f\"  - Sex: Male\")\n",
        "print(f\"  - Age: Youngest decade in data\")\n",
        "print(f\"  - Language family: Germanic (includes English)\")\n",
        "print(f\"  - Language background: English only (monolingual)\")\n",
        "print(f\"\\nFeature names: {feature_names}\")\n",
        "print()\n",
        "print(ols_model.summary(xname=feature_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc32967",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae547e6",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118b19f1",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
